{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorrivin/svm-pytorch/blob/master/convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RyGk3Anp27o3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './my_graph/mnist'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_GZKH-V13EDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iw5pf63p3Kg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d213d07-4cdd-4aef-c87e-8fd40cbbeafe"
      },
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://c3d39de5.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5mIyQhG8HPwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "477f0280-9cf6-4418-940c-41636736734f"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" Using convolutional net on MNIST dataset of handwritten digit\n",
        "(http://yann.lecun.com/exdb/mnist/)\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import time\n",
        "import utils\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "N_CLASSES = 10\n",
        "\n",
        "# Step 1: Read in data\n",
        "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
        "mnist = input_data.read_data_sets(\"/data/mnist\", one_hot=True)\n",
        "\n",
        "# Step 2: Define paramaters for the model\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "SKIP_STEP = 10\n",
        "DROPOUT = 0.75\n",
        "N_EPOCHS = 1\n",
        "\n",
        "# Step 3: create placeholders for features and labels\n",
        "# each image in the MNIST data is of shape 28*28 = 784\n",
        "# therefore, each image is represented with a 1x784 tensor\n",
        "# We'll be doing dropout for hidden layer so we'll need a placeholder\n",
        "# for the dropout probability too\n",
        "# Use None for shape so we can change the batch_size once we've built the graph\n",
        "with tf.name_scope('data'):\n",
        "    X = tf.placeholder(tf.float32, [None, 784], name=\"X_placeholder\")\n",
        "    Y = tf.placeholder(tf.float32, [None, 10], name=\"Y_placeholder\")\n",
        "\n",
        "dropout = tf.placeholder(tf.float32, name='dropout')\n",
        "\n",
        "# Step 4 + 5: create weights + do inference\n",
        "# the model is conv -> relu -> pool -> conv -> relu -> pool -> fully connected -> softmax\n",
        "\n",
        "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
        "\n",
        "import pathlib\n",
        "pathlib.Path('checkpoints').mkdir(parents=True, exist_ok=True) \n",
        "pathlib.Path('checkpoints/convnet_mnist').mkdir(parents=True, exist_ok=True) \n",
        "#utils.make_dir('checkpoints')\n",
        "#utils.make_dir('checkpoints/convnet_mnist')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-917d2dcf3793>:23: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /data/mnist/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting /data/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dBb8IzZUHiJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c638c128-5fc1-4473-db20-1308f90274da"
      },
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7OXHcrU0H-Ve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('conv1') as scope:\n",
        "    # first, reshape the image to [BATCH_SIZE, 28, 28, 1] to make it work with tf.nn.conv2d\n",
        "    # use the dynamic dimension -1\n",
        "    images = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    # TO DO\n",
        "    size_in = 1\n",
        "    size_out =32\n",
        "    # create kernel variable of dimension [5, 5, 1, 32]\n",
        "    # use tf.truncated_normal_initializer()\n",
        "    w = tf.get_variable('W',shape=[5, 5, size_in, size_out], initializer = tf.truncated_normal_initializer())\n",
        "    \n",
        "\n",
        "    # TO DO\n",
        "\n",
        "    # create biases variable of dimension [32]\n",
        "    # use tf.constant_initializer(0.0)\n",
        "    #b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
        "    b = tf.get_variable(\"B\",shape=[size_out], dtype=tf.float32, trainable = True, initializer = tf.constant_initializer(0.0))\n",
        "    # TO DO\n",
        "\n",
        "    # apply tf.nn.conv2d. strides [1, 1, 1, 1], padding is 'SAME'\n",
        "    conv = tf.nn.conv2d(images, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "    # TO DO\n",
        "   \n",
        "    # apply relu on the sum of convolution output and biases\n",
        "\n",
        "    # TO DO\n",
        "    conv1 = tf.nn.relu(conv + b, name=scope.name)\n",
        "    tf.summary.histogram(\"weights\", w)\n",
        "    tf.summary.histogram(\"biases\", b)\n",
        "    tf.summary.histogram(\"activations\", conv1)\n",
        "    # output is of dimension BATCH_SIZE x 28 x 28 x 32\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a6HDvzD9K4bD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('pool1') as scope:\n",
        "  # apply max pool with ksize [1, 2, 2, 1], and strides [1, 2, 2, 1], padding 'SAME'\n",
        "\n",
        "  # TO DO\n",
        "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\", name=scope.name)\n",
        "  # output is of dimension BATCH_SIZE x 14 x 14 x 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8k4HkKZOBll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('conv2') as scope:\n",
        "    # similar to conv1, except kernel now is of the size 5 x 5 x 32 x 64\n",
        "    kernel = tf.get_variable('kernels', [5, 5, 32, 64],\n",
        "                             initializer=tf.truncated_normal_initializer())\n",
        "    biases = tf.get_variable('biases', [64],\n",
        "                             initializer=tf.random_normal_initializer())\n",
        "    conv = tf.nn.conv2d(pool1, kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    conv2 = tf.nn.relu(conv + biases, name=scope.name)\n",
        "\n",
        "    # output is of dimension BATCH_SIZE x 14 x 14 x 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HxpmrHA_OCVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('pool2') as scope:\n",
        "    # similar to pool1\n",
        "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
        "                           padding='SAME', name=scope.name)\n",
        "\n",
        "    # output is of dimension BATCH_SIZE x 7 x 7 x 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c-9DqnfZOGmm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('fc') as scope:\n",
        "    # use weight of dimension 7 * 7 * 64 x 1024\n",
        "    input_features = 7 * 7 * 64\n",
        "    size_out=1024\n",
        "\n",
        "    # create weights and biases\n",
        "    w = tf.Variable(tf.truncated_normal([input_features, size_out], stddev=0.1), name=\"W\")\n",
        "    b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
        "    # TO DO\n",
        "\n",
        "    # reshape pool2 to 2 dimensional\n",
        "    pool2 = tf.reshape(pool2, [-1, input_features])\n",
        "\n",
        "    # apply relu on matmul of pool2 and w + b\n",
        "    fc = tf.nn.relu(tf.matmul(pool2, w) + b, name='relu')\n",
        "\n",
        "    # TO DO\n",
        "    tf.summary.histogram(\"weights\", w)\n",
        "    tf.summary.histogram(\"biases\", b)\n",
        "    tf.summary.histogram(\"activations\", fc)\n",
        "    # apply dropout\n",
        "    fc = tf.nn.dropout(fc, dropout, name='relu_dropout')\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWoLGPYGVl63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('softmax_linear') as scope:\n",
        "  # this you should know. get logits without softmax\n",
        "  # you need to create weights and biases\n",
        "  w = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1), name=\"W\")\n",
        "  b = tf.Variable(tf.constant(0.1, shape=[10]), name=\"B\")\n",
        "  logits = fc@w + b\n",
        "  # TO DO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iS93D0VMXMkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "5683b267-2ba9-46af-9caa-4361edb14424"
      },
      "cell_type": "code",
      "source": [
        "# Step 6: define loss function\n",
        "# use softmax cross entropy with logits as the loss function\n",
        "# compute mean cross entropy, softmax is applied internally\n",
        "with tf.name_scope('loss'):\n",
        "  # you should know how to do this too\n",
        "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits), name=scope.name)\n",
        "  tf.summary.scalar(\"loss\", loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-f5f7f0b72e92>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E1BtDKE3X6wP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbF1rHuAYyf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "11c8502f-16b5-4273-bc54-3b227ad31f82"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver()\n",
        "    # to visualize using TensorBoard\n",
        "    writer = tf.summary.FileWriter('./my_graph/mnist')\n",
        "    writer.add_graph(sess.graph)\n",
        "    ##### You have to create folders to store checkpoints\n",
        "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
        "    # if that checkpoint exists, restore from checkpoint\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "    summ = tf.summary.merge_all()\n",
        "    sess.run(global_step.initializer)\n",
        "    initial_step = global_step.eval()\n",
        "\n",
        "    start_time = time.time()\n",
        "    n_batches = int(mnist.train.num_examples / BATCH_SIZE)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for index in range(initial_step, n_batches * N_EPOCHS):  # train the model n_epochs times\n",
        "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
        "        _, loss_batch = sess.run([optimizer, loss],\n",
        "                                 feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n",
        "        total_loss += loss_batch\n",
        "        if (index + 1) % SKIP_STEP == 0:\n",
        "            print('Average loss at step {}: {:5.1f}'.format(index + 1, total_loss / SKIP_STEP))\n",
        "            total_loss = 0.0\n",
        "            saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', index)\n",
        "\n",
        "    print(\"Optimization Finished!\")  # should be around 0.35 after 25 epochs\n",
        "    print(\"Total time: {0} seconds\".format(time.time() - start_time))\n",
        "\n",
        "    # test the model\n",
        "    n_batches = int(mnist.test.num_examples / BATCH_SIZE)\n",
        "    total_correct_preds = 0\n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(BATCH_SIZE)\n",
        "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits],\n",
        "                                               feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n",
        "        s = sess.run(summ,feed_dict={X: X_batch, Y: Y_batch, dropout: DROPOUT})\n",
        "        writer.add_summary(s, i)\n",
        "        preds = tf.nn.softmax(logits_batch)\n",
        "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
        "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "        total_correct_preds += sess.run(accuracy)\n",
        "\n",
        "    print(\"Accuracy {0}\".format(total_correct_preds / mnist.test.num_examples))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints/convnet_mnist/mnist-convnet-419\n",
            "Average loss at step 10:   1.0\n",
            "Average loss at step 20:   0.9\n",
            "Average loss at step 30:   0.9\n",
            "Average loss at step 40:   0.7\n",
            "Average loss at step 50:   1.0\n",
            "Average loss at step 60:   1.1\n",
            "Average loss at step 70:   0.9\n",
            "Average loss at step 80:   0.8\n",
            "Average loss at step 90:   1.0\n",
            "Average loss at step 100:   1.2\n",
            "Average loss at step 110:   1.2\n",
            "Average loss at step 120:   0.9\n",
            "Average loss at step 130:   0.9\n",
            "Average loss at step 140:   1.1\n",
            "Average loss at step 150:   0.9\n",
            "Average loss at step 160:   1.0\n",
            "Average loss at step 170:   0.8\n",
            "Average loss at step 180:   1.2\n",
            "Average loss at step 190:   0.9\n",
            "Average loss at step 200:   0.9\n",
            "Average loss at step 210:   1.2\n",
            "Average loss at step 220:   0.9\n",
            "Average loss at step 230:   0.8\n",
            "Average loss at step 240:   0.9\n",
            "Average loss at step 250:   1.0\n",
            "Average loss at step 260:   0.9\n",
            "Average loss at step 270:   0.9\n",
            "Average loss at step 280:   1.0\n",
            "Average loss at step 290:   0.9\n",
            "Average loss at step 300:   0.9\n",
            "Average loss at step 310:   0.7\n",
            "Average loss at step 320:   1.0\n",
            "Average loss at step 330:   1.0\n",
            "Average loss at step 340:   0.9\n",
            "Average loss at step 350:   0.8\n",
            "Average loss at step 360:   0.9\n",
            "Average loss at step 370:   1.1\n",
            "Average loss at step 380:   1.0\n",
            "Average loss at step 390:   0.8\n",
            "Average loss at step 400:   0.8\n",
            "Average loss at step 410:   0.8\n",
            "Average loss at step 420:   0.9\n",
            "Optimization Finished!\n",
            "Total time: 9.623056411743164 seconds\n",
            "Accuracy 0.8988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tw5ZPcN1ZVEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBUU-Ihk1YNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-hm6FaW1m52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUwx9zdB1pyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}